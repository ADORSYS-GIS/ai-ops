{{- if or .Values.controllers.cronjob.enabled .Values.controllers.job.enabled }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "cnpg-backup.scriptsConfigMapName" . }}
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "cnpg-backup.labels" . | nindent 4 }}
  annotations:
    description: "Backup and restore scripts for CNPG database"
data:
  backup.sh: |
    #!/bin/bash
    set -Eeuo pipefail

    ###############################################################################
    # PostgreSQL Backup Script for CloudNativePG
    # Features: Pre-backup health check, S3 upload with endpoint support,
    #           proper error handling, and logging
    ###############################################################################

    readonly SCRIPT_NAME="$(basename "$0")"
    readonly TIMESTAMP="$(date +'%Y-%m-%d_%H-%M-%S')"
    readonly LOG_FILE="/tmp/backup_${TIMESTAMP}.log"

    ###############################################################################
    # Logging functions
    ###############################################################################
    log() {
        local level="$1"
        shift
        local message="$*"
        local timestamp
        timestamp="$(date '+%Y-%m-%d %H:%M:%S')"
        echo "[${timestamp}] [${level}] ${message}" | tee -a "${LOG_FILE}"
    }

    log_info()    { log "INFO"    "$@"; }
    log_warning() { log "WARNING" "$@"; }
    log_error()   { log "ERROR"   "$@"; }
    log_success() { log "SUCCESS" "$@"; }

    ###############################################################################
    # Signal handlers for cleanup
    ###############################################################################
    cleanup() {
    local exit_code=$?
    # Clean up all backup files matching pattern
    find /tmp -name "backup_*.log" -o -name "*_${TIMESTAMP}.sql.gz" 2>/dev/null | while read -r file; do
        rm -f "$file" 2>/dev/null || true
    done
    if [[ $exit_code -ne 0 ]]; then
        log_error "Backup failed with exit code ${exit_code}"
        log_error "See ${LOG_FILE} for details"
    fi
    exit $exit_code
    }
    trap cleanup ERR EXIT INT TERM

    ###############################################################################
    # Validate environment
    ###############################################################################
    validate_environment() {
    log_info "Validating backup environment..."

    # Validate required environment variables
    : "${PGHOST:?PGHOST environment variable is not set}"
    : "${PGPORT:?PGPORT environment variable is not set}"
    : "${PGDATABASE:?PGDATABASE environment variable is not set}"
    : "${PGUSER:?PGUSER environment variable is not set}"
    : "${PGPASSWORD:?PGPASSWORD environment variable is not set}"

    # Validate S3 settings
    : "${AWS_ACCESS_KEY_ID:?AWS_ACCESS_KEY_ID environment variable is not set}"
    : "${AWS_SECRET_ACCESS_KEY:?AWS_SECRET_ACCESS_KEY environment variable is not set}"
    
    # Validate credentials are not empty
    if [[ -z "${AWS_ACCESS_KEY_ID}" || "${AWS_ACCESS_KEY_ID}" =~ ^[[:space:]]*$ ]]; then
        log_error "AWS_ACCESS_KEY_ID is empty"
        exit 1
    fi
    if [[ -z "${AWS_SECRET_ACCESS_KEY}" || "${AWS_SECRET_ACCESS_KEY}" =~ ^[[:space:]]*$ ]]; then
        log_error "AWS_SECRET_ACCESS_KEY is empty"
        exit 1
    fi

    # Export AWS region if set
    {{- if .Values.s3.region }}
    export AWS_DEFAULT_REGION="{{ .Values.s3.region }}"
    export AWS_REGION="{{ .Values.s3.region }}"
    {{- end }}

    log_info "Environment validation passed"
    }

    ###############################################################################
    # Check PostgreSQL connectivity
    ###############################################################################
    check_pg_connectivity() {
        log_info "Checking PostgreSQL connectivity..."
        log_info "Host: ${PGHOST}, Port: ${PGPORT}, Database: ${PGDATABASE}"

        # Wait for PostgreSQL to be ready
        local max_attempts=30
        local attempt=0
        local pg_ready=false

        while [[ $attempt -lt $max_attempts ]]; do
            if pg_isready -h "${PGHOST}" -p "${PGPORT}" -q 2>/dev/null; then
                pg_ready=true
                break
            fi
            attempt=$((attempt + 1))
            log_warning "PostgreSQL not ready (attempt ${attempt}/${max_attempts}), retrying in 5s..."
            sleep 5
        done

        if [[ "$pg_ready" != "true" ]]; then
            log_error "PostgreSQL is not responding after ${max_attempts} attempts"
            return 1
        fi

        # Additional check - try to execute a query
        if PGPASSWORD="${PGPASSWORD}" psql -h "${PGHOST}" -p "${PGPORT}" -U "${PGUSER}" -d "${PGDATABASE}" -c "SELECT 1;" -q 2>/dev/null; then
            log_success "PostgreSQL is healthy and ready for backup"
        else
            log_error "Cannot execute queries on PostgreSQL"
            return 1
        fi
    }

    ###############################################################################
    # Perform database backup
    ###############################################################################
    perform_backup() {
        local db_name="${PGDATABASE}"
        local backup_file="/tmp/${db_name}_${TIMESTAMP}.sql.gz"
        local s3_bucket="{{ .Values.s3.bucket }}"
        local s3_prefix="{{ .Values.s3.prefix }}"
        {{- if .Values.s3.endpoint }}
        local s3_endpoint="{{ .Values.s3.endpoint }}"
        {{- end }}

        log_info "Starting database backup..."
        log_info "Database: ${db_name}"
        log_info "Backup file: ${backup_file}"

        # Get database size for logging
        local db_size
        db_size=$(PGPASSWORD="${PGPASSWORD}" psql -h "${PGHOST}" -p "${PGPORT}" -U "${PGUSER}" -d "${db_name}" -t -c "SELECT pg_size_pretty(pg_database_size('${db_name}'));" 2>/dev/null | xargs || echo "unknown")
        log_info "Database size: ${db_size}"

        # Perform pg_dump with compression
        log_info "Running pg_dump..."
        local start_time
        start_time=$(date +%s)

        # Suppress NOTICE messages
        export PGOPTIONS='-c client_min_messages=warning'

        if PGPASSWORD="${PGPASSWORD}" pg_dump \
            --host="${PGHOST}" \
            --port="${PGPORT}" \
            --username="${PGUSER}" \
            --dbname="${db_name}" \
            --no-owner \
            --no-acl \
            --compress=9 \
            2>> "${LOG_FILE}" | gzip > "${backup_file}"; then
            local end_time
            end_time=$(date +%s)
            local duration=$((end_time - start_time))

            if [[ -s "${backup_file}" ]]; then
                local file_size
                file_size=$(du -h "${backup_file}" | cut -f1)
                log_success "Backup completed successfully"
                log_info "File size: ${file_size}"
                log_info "Duration: ${duration}s"
            else
                log_error "Backup file is empty or missing"
                return 1
            fi
        else
            log_error "pg_dump failed"
            return 1
        fi
    }

    ###############################################################################
    # Upload backup to S3
    ###############################################################################
    upload_to_s3() {
        local backup_file="$1"
        local s3_bucket="{{ .Values.s3.bucket }}"
        local s3_prefix="{{ .Values.s3.prefix }}"
        {{- if .Values.s3.endpoint }}
        local s3_endpoint="{{ .Values.s3.endpoint }}"
        {{- end }}

        local filename
        filename=$(basename "${backup_file}")
        local s3_path="s3://${s3_bucket}/${s3_prefix}/${filename}"

        log_info "Uploading backup to S3..."
        log_info "S3 path: ${s3_path}"

        local start_time
        start_time=$(date +%s)

        # Build AWS CLI command with optional endpoint
        local aws_cmd=("aws" "s3" "cp" "${backup_file}" "${s3_path}")
        {{- if .Values.s3.endpoint }}
        aws_cmd+=("--endpoint-url" "${s3_endpoint}")
        {{- if or (contains "minio" .Values.s3.endpoint) (contains "s3." .Values.s3.endpoint) }}
        aws_cmd+=("--no-verify-ssl")
        {{- end }}
        {{- end }}
        {{- if .Values.s3.storageClass }}
        aws_cmd+=("--storage-class" "{{ .Values.s3.storageClass }}")
        {{- end }}

        if "${aws_cmd[@]}" 2>> "${LOG_FILE}"; then
            local end_time
            end_time=$(date +%s)
            local duration=$((end_time - start_time))
            log_success "Upload completed successfully"
            log_info "Duration: ${duration}s"

            # Verify upload
            local verify_cmd=("aws" "s3" "ls" "${s3_path}")
            {{- if .Values.s3.endpoint }}
            verify_cmd+=("--endpoint-url" "${s3_endpoint}")
            {{- if or (contains "minio" .Values.s3.endpoint) (contains "s3." .Values.s3.endpoint) }}
            verify_cmd+=("--no-verify-ssl")
            {{- end }}
            {{- end }}
            
            if "${verify_cmd[@]}" >/dev/null 2>&1; then
                log_info "Backup verified in S3"
            else
                log_warning "Could not verify backup in S3"
            fi
        else
            log_error "Failed to upload backup to S3"
            return 1
        fi
    }

    ###############################################################################
    # Cleanup old backups (best-effort, S3 lifecycle rules preferred)
    ###############################################################################
    cleanup_old_backups() {
    {{- if and (.Values.backup.retentionDays) (gt (.Values.backup.retentionDays | int) 0) }}
    local s3_bucket="{{ .Values.s3.bucket }}"
    local s3_prefix="{{ .Values.s3.prefix }}"
    local retention_days="{{ .Values.backup.retentionDays }}"

    log_info "Cleaning up backups older than ${retention_days} days (best-effort)..."

    # Use ISO 8601 date format for portability
    local cutoff_date
    cutoff_date=$(date -u -d "-${retention_days} days" +'%Y-%m-%dT%H:%M:%SZ' 2>/dev/null || date -u -v-${retention_days}d +'%Y-%m-%dT%H:%M:%SZ' 2>/dev/null || echo "")
    
    if [[ -z "$cutoff_date" ]]; then
        log_warning "Cannot calculate cutoff date, skipping cleanup"
        return
    fi

    # List objects and filter by date using aws s3api for proper date comparison
    local old_backups
    local list_cmd=("aws" "s3api" "list-objects-v2" "--bucket" "${s3_bucket}" "--prefix" "${s3_prefix}/" "--query" "Contents[?LastModified<'${cutoff_date}'].[Key]" "--output" "text")
    {{- if .Values.s3.endpoint }}
    list_cmd+=("--endpoint-url" "${s3_endpoint}")
    {{- if or (contains "minio" .Values.s3.endpoint) (contains "s3." .Values.s3.endpoint) }}
    list_cmd+=("--no-verify-ssl")
    {{- end }}
    {{- end }}
    
    if ! old_backups=$("${list_cmd[@]}" 2>> "${LOG_FILE}"); then
      log_warning "Could not list old backups from S3. Skipping cleanup."
      return
    fi

    echo "${old_backups}" | while read -r key; do
        if [[ -n "$key" ]]; then
            log_info "Removing old backup: ${key}"
            local rm_cmd=("aws" "s3" "rm" "s3://${s3_bucket}/${key}")
            {{- if .Values.s3.endpoint }}
            rm_cmd+=("--endpoint-url" "${s3_endpoint}")
            {{- if or (contains "minio" .Values.s3.endpoint) (contains "s3." .Values.s3.endpoint) }}
            rm_cmd+=("--no-verify-ssl")
            {{- end }}
            {{- end }}
            if ! "${rm_cmd[@]}" 2>> "${LOG_FILE}"; then
                log_warning "Failed to remove old backup: ${key}"
            fi
        fi
    done
    {{- else }}
    log_info "Retention cleanup disabled (use S3 lifecycle rules for production)"
    {{- end }}
    }
    ###############################################################################
    # Main execution
    ###############################################################################
    main() {
        log_info "========================================"
        log_info "PostgreSQL Backup Started"
        log_info "========================================"

        # Validate environment
        validate_environment

        # Check PostgreSQL connectivity
        check_pg_connectivity

        # Perform backup
        local backup_file="/tmp/${PGDATABASE}_${TIMESTAMP}.sql.gz"
        perform_backup

        # Upload to S3
        upload_to_s3 "${backup_file}"

        # Cleanup old backups (optional)
        cleanup_old_backups

        log_info "========================================"
        log_success "Backup completed successfully"
        log_info "========================================"
    }

    main "$@"

  restore.sh: |
    #!/bin/bash
    set -Eeuo pipefail

    ###############################################################################
    # PostgreSQL Restore Script for CloudNativePG
    # Features: Pre-restore health check, safety checks, S3 download with endpoint
    #           support, and comprehensive logging
    ###############################################################################

    readonly SCRIPT_NAME="$(basename "$0")"
    readonly TIMESTAMP="$(date +'%Y-%m-%d_%H-%M-%S')"
    readonly LOG_FILE="/tmp/restore_${TIMESTAMP}.log"

    ###############################################################################
    # Logging functions
    ###############################################################################
    log() {
        local level="$1"
        shift
        local message="$*"
        local timestamp
        timestamp="$(date '+%Y-%m-%d %H:%M:%S')"
        echo "[${timestamp}] [${level}] ${message}" | tee -a "${LOG_FILE}"
    }

    log_info()    { log "INFO"    "$@"; }
    log_warning() { log "WARNING" "$@"; }
    log_error()   { log "ERROR"   "$@"; }
    log_success() { log "SUCCESS" "$@"; }

    ###############################################################################
    # Signal handlers for cleanup
    ###############################################################################
    cleanup() {
        local exit_code=$?
        # Clean up temp files
        rm -f /tmp/restore_*.sql.gz /tmp/restore_*.sql 2>/dev/null || true
        if [[ $exit_code -ne 0 ]]; then
            log_error "Restore failed with exit code ${exit_code}"
            log_error "See ${LOG_FILE} for details"
        fi
        exit $exit_code
    }
    trap cleanup ERR EXIT INT TERM

    ###############################################################################
    # Validate environment
    ###############################################################################
    validate_environment() {
        log_info "Validating restore environment..."

        # Validate required environment variables
        : "${PGHOST:?PGHOST environment variable is not set}"
        : "${PGPORT:?PGPORT environment variable is not set}"
        : "${PGDATABASE:?PGDATABASE environment variable is not set}"
        : "${PGUSER:?PGUSER environment variable is not set}"
        : "${PGPASSWORD:?PGPASSWORD environment variable is not set}"

        # Validate S3 settings
        : "${AWS_ACCESS_KEY_ID:?AWS_ACCESS_KEY_ID environment variable is not set}"
        : "${AWS_SECRET_ACCESS_KEY:?AWS_SECRET_ACCESS_KEY environment variable is not set}"

        # Export AWS region if set
        {{- if .Values.s3.region }}
        export AWS_DEFAULT_REGION="{{ .Values.s3.region }}"
        export AWS_REGION="{{ .Values.s3.region }}"
        {{- end }}

        log_info "Environment validation passed"
    }

    ###############################################################################
    # Check PostgreSQL connectivity
    ###############################################################################
    check_pg_connectivity() {
        log_info "Checking PostgreSQL connectivity..."
        log_info "Host: ${PGHOST}, Port: ${PGPORT}, Database: ${PGDATABASE}"

        # Wait for PostgreSQL to be ready
        local max_attempts=60
        local attempt=0
        local pg_ready=false

        while [[ $attempt -lt $max_attempts ]]; do
            if pg_isready -h "${PGHOST}" -p "${PGPORT}" -q 2>/dev/null; then
                pg_ready=true
                break
            fi
            attempt=$((attempt + 1))
            log_warning "PostgreSQL not ready (attempt ${attempt}/${max_attempts}), retrying in 5s..."
            sleep 5
        done

        if [[ "$pg_ready" != "true" ]]; then
            log_error "PostgreSQL is not responding after ${max_attempts} attempts"
            return 1
        fi

        log_success "PostgreSQL is ready"
    }

    ###############################################################################
    # Check if database is empty (safety check)
    ###############################################################################
    check_database_empty() {
    log_info "Checking if database is empty..."

    local table_count
    table_count=$(PGPASSWORD="${PGPASSWORD}" psql -h "${PGHOST}" -p "${PGPORT}" -U "${PGUSER}" -d "${PGDATABASE}" -t -c "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public';" 2>/dev/null | xargs || echo "0")

    if [[ "$table_count" -gt 0 ]]; then
        log_warning "Database contains ${table_count} tables"
        if [[ "${FORCE_RESTORE:-false}" == "true" ]]; then
            log_warning "FORCE mode enabled - proceeding with restore"
        else
            log_error "Database is not empty. Set restore.force=true to proceed anyway."
            log_error "WARNING: This will DROP all existing objects!"
            return 1
        fi
    else
        log_info "Database is empty (${table_count} tables)"
    fi
    }

    ###############################################################################
    # Download backup from S3
    ###############################################################################
    download_from_s3() {
        local s3_object="$1"
        local s3_bucket="{{ .Values.s3.bucket }}"
        local s3_prefix="{{ .Values.s3.prefix }}"
        {{- if .Values.s3.endpoint }}
        local s3_endpoint="{{ .Values.s3.endpoint }}"
        {{- end }}

        local s3_path="s3://${s3_bucket}/${s3_prefix}/${s3_object}"
        local local_file="/tmp/restore_${s3_object}"

        log_info "Downloading backup from S3..."
        log_info "S3 path: ${s3_path}"

        local start_time
        start_time=$(date +%s)

        # Build AWS CLI command with optional endpoint
        local aws_cmd=("aws" "s3" "cp" "${s3_path}" "${local_file}")
        {{- if .Values.s3.endpoint }}
        aws_cmd+=("--endpoint-url" "${s3_endpoint}")
        {{- if or (contains "minio" .Values.s3.endpoint) (contains "s3." .Values.s3.endpoint) }}
        aws_cmd+=("--no-verify-ssl")
        {{- end }}
        {{- end }}

        if "${aws_cmd[@]}" 2>> "${LOG_FILE}"; then
            local end_time
            end_time=$(date +%s)
            local duration=$((end_time - start_time))
            log_success "Download completed successfully"
            log_info "Duration: ${duration}s"

            if [[ -s "${local_file}" ]]; then
                local file_size
                file_size=$(du -h "${local_file}" | cut -f1)
                log_info "File size: ${file_size}"
                echo "${local_file}"
            else
                log_error "Downloaded file is empty or missing"
                return 1
            fi
        else
            log_error "Failed to download backup from S3"
            return 1
        fi
    }

    ###############################################################################
    # Perform database restore
    ###############################################################################
    perform_restore() {
        local backup_file="$1"
        local db_name="${PGDATABASE}"

        log_info "Starting database restore..."
        log_info "Database: ${db_name}"
        log_info "Backup file: ${backup_file}"

        local start_time
        start_time=$(date +%s)

        # For gzip files, decompress and pipe to psql
        if gzip -t "${backup_file}" 2>/dev/null; then
            log_info "Decompressing backup..."

            # Check if it's a valid gzip file
            if gzip -t "${backup_file}" 2>/dev/null; then
                log_info "Restoring database from compressed backup..."
                if gunzip -c "${backup_file}" | PGPASSWORD="${PGPASSWORD}" psql \
                    --host="${PGHOST}" \
                    --port="${PGPORT}" \
                    --username="${PGUSER}" \
                    --dbname="${db_name}" 2>> "${LOG_FILE}"; then
                    local end_time
                    end_time=$(date +%s)
                    local duration=$((end_time - start_time))
                    log_success "Restore completed successfully"
                    log_info "Duration: ${duration}s"
                else
                    log_error "Restore failed"
                    return 1
                fi
            else
                log_error "Invalid gzip file"
                return 1
            fi
        else
            log_error "Unsupported backup file format"
            return 1
        fi
    }

    ###############################################################################
    # Verify restore
    ###############################################################################
    verify_restore() {
        log_info "Verifying restore..."

        # Check if we can connect and get table count
        local table_count
        table_count=$(PGPASSWORD="${PGPASSWORD}" psql -h "${PGHOST}" -p "${PGPORT}" -U "${PGUSER}" -d "${PGDATABASE}" -t -c "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public';" 2>/dev/null | xargs || echo "0")

        if [[ "$table_count" -gt 0 ]]; then
            log_success "Restore verified: ${table_count} tables found in database"
        else
            log_warning "Restore completed but no tables found (may be normal for empty databases)"
        fi
    }

    ###############################################################################
    # Main execution
    ###############################################################################
    main() {
        local s3_object="${1:-}"

        log_info "========================================"
        log_info "PostgreSQL Restore Started"
        log_info "========================================"

        # Validate arguments
        if [[ -z "${s3_object}" ]]; then
            log_error "Usage: restore.sh <s3-object>"
            log_error "Example: restore.sh appdb_2026-01-21_02-00-00.sql.gz"
            return 1
        fi

        log_info "Restore object: ${s3_object}"

        # Validate environment
        validate_environment

        # Check PostgreSQL connectivity
        check_pg_connectivity

        # Check if database is empty (safety check)
        check_database_empty

        # Download backup from S3
        local backup_file
        backup_file=$(download_from_s3 "${s3_object}")

        # Perform restore
        perform_restore "${backup_file}"

        # Verify restore
        verify_restore

        # Cleanup
        rm -f "${backup_file}" 2>/dev/null || true

        log_info "========================================"
        log_success "Restore completed successfully"
        log_info "========================================"
    }

    main "$@"
{{- end }}