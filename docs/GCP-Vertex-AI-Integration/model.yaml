apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: qwen-secondary
  namespace: default
spec:
  predictor:
    containers:
      - name: kserve-container
        image: ollama/ollama:latest
        ports:
          - containerPort: 11434
            protocol: TCP
        command: ["/bin/bash", "-c"]
        args:
          - "ollama serve & sleep 5 && ollama pull qwen2.5:0.5b && wait"
        resources:
          requests:
            cpu: "1"
            memory: "2Gi"
          limits:
            cpu: "2"
            memory: "2Gi"
