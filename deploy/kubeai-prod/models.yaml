---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: kubeai-models
  namespace: argocd
spec:
  project: application
  source:
    repoURL: https://www.kubeai.org
    chart: models
    targetRevision: 0.20.0
    helm:
      valuesObject:
        catalog:
          gemma-3-27b-ollama-l4:
            enabled: true
            engine: VLLM
            resourceProfile: nvidia-gpu-l4:4
          llama-3.3-70b-instruct-fp8-l4:
            enabled: true
            features: [TextGeneration]
            url: hf://meta-llama/Llama-3.3-70B-Instruct
            engine: VLLM
            env:
              VLLM_ATTENTION_BACKEND: FLASHINFER
            args:
              - --max-model-len=32768
              - --max-num-batched-token=32768
              - --max-num-seqs=512
              - --gpu-memory-utilization=0.98
              # Pipeline parallelism performs better than tensor over PCI.
              - --pipeline-parallel-size=4
              # A minimum of tensor parallel 2 was needed to not have OOM errors.
              # We use 8 GPUs so parallelism strategy of 4 x 2 works well.
              - --tensor-parallel-size=2
              - --enable-prefix-caching
              - --enable-chunked-prefill=false
              - --disable-log-requests
              - --kv-cache-dtype=fp8
              # Enforce eager wasn't supported with FLASHINFER.
              - --enforce-eager
          qwen2-5-coder-32b-instruct-fp16-l4:
            enabled: true
            features: [TextGeneration]
            url: hf://Qwen/Qwen2.5-Coder-32B-Instruct
            engine: VLLM
            env:
              VLLM_ATTENTION_BACKEND: FLASHINFER
            args:
              - --max-model-len=67584
              - --max-num-batched-token=67584
              - --max-num-seqs=512
              - --gpu-memory-utilization=0.98
              # Pipeline parallelism performs better than tensor over PCI.
              - --pipeline-parallel-size=2
              # A minimum of tensor parallel 2 was needed to not have OOM errors.
              # We use 4 GPUs so parallelism strategy of 2 x 2 works well.
              - --tensor-parallel-size=2
              - --enable-prefix-caching
              - --enable-chunked-prefill=false
              - --disable-log-requests
              - --kv-cache-dtype=fp8
              # Enforce eager wasn't supported with FLASHINFER.
              - --enforce-eager
            resourceProfile: nvidia-gpu-l4:4
          devstral-small-2505-fp16-l4:
            enabled: true
            features: [TextGeneration]
            url: hf://mistralai/Devstral-Small-2505
            engine: VLLM
            env:
              VLLM_ATTENTION_BACKEND: FLASHINFER
            args:
              - --max-model-len=48128
              - --max-num-batched-token=48128
              - --max-num-seqs=512
              - --gpu-memory-utilization=0.98
              # Pipeline parallelism performs better than tensor over PCI.
              - --pipeline-parallel-size=2
              # A minimum of tensor parallel 2 was needed to not have OOM errors.
              # We use 8 GPUs so parallelism strategy of 2 x 2 works well.
              - --tensor-parallel-size=2
              - --enable-prefix-caching
              - --enable-chunked-prefill=false
              - --disable-log-requests
              - --kv-cache-dtype=fp8
              # Enforce eager wasn't supported with FLASHINFER.
              - --enforce-eager
            resourceProfile: nvidia-gpu-l4:4
          llama-3.1-8b-instruct-fp8-l4:
            enabled: true
          qwen2.5-coder-1.5b-a10:
            enabled: true
            features: [ "TextGeneration" ]
            url: "hf://Qwen/Qwen2.5-Coder-1.5B-Instruct"
            engine: VLLM
            env:
              VLLM_ATTENTION_BACKEND: FLASHINFER
            args:
              - --max-model-len=2048
              - --max-num-seqs=16
              - --quantization=fp8
              - --kv-cache-dtype=fp8
            resourceProfile: nvidia-gpu-l4:1
            minReplicas: 0
          qwen2-5-coder-7b-instruct-fp16-l4:
            enabled: true
            features: [TextGeneration]
            url: hf://Qwen/Qwen2.5-Coder-7B-Instruct
            engine: VLLM
            env:
              VLLM_ATTENTION_BACKEND: FLASHINFER
            args:
              - --max-model-len=15360
              - --max-num-batched-token=15360
              - --max-num-seqs=512
              - --gpu-memory-utilization=0.98
              # Pipeline parallelism performs better than tensor over PCI.
              - --pipeline-parallel-size=2
              # A minimum of tensor parallel 2 was needed to not have OOM errors.
              # We use 8 GPUs so parallelism strategy of 2 x 2 works well.
              - --tensor-parallel-size=1
              - --enable-prefix-caching
              - --enable-chunked-prefill=false
              - --disable-log-requests
              - --kv-cache-dtype=fp8
              # Enforce eager wasn't supported with FLASHINFER.
              - --enforce-eager
            resourceProfile: nvidia-gpu-l4:2
          nomic-embed-text-cpu:
            enabled: true
  
  destination:
    server: https://kubernetes.default.svc
    namespace: kubeai
  
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
  
  revisionHistoryLimit: 3