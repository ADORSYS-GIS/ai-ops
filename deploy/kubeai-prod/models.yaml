---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: kubeai-models
  namespace: argocd
spec:
  project: application
  source:
    repoURL: https://www.kubeai.org
    chart: models
    targetRevision: 0.20.0
    helm:
      valuesObject:
        catalog:
          gemma-3-27b-ollama-l4:
            enabled: true
            #engine: VLLM
            #resourceProfile: nvidia-gpu-l4:1
            #minReplicas: 1 # by default this is 0
          llama-3.1-70b-instruct-fp8-l4:
            enabled: true
          llama-3.3-70b-instruct-fp8-l4:
            enabled: true
            features: [TextGeneration]
            url: hf://meta-llama/Llama-3.3-70B-Instruct
            engine: VLLM
            env:
              VLLM_ATTENTION_BACKEND: FLASHINFER
            args:
              - --max-model-len=32768
              - --max-num-batched-token=32768
              - --max-num-seqs=512
              - --gpu-memory-utilization=0.98
              # Pipeline parallelism performs better than tensor over PCI.
              - --pipeline-parallel-size=4
              # A minimum of tensor parallel 2 was needed to not have OOM errors.
              # We use 8 GPUs so parallelism strategy of 4 x 2 works well.
              - --tensor-parallel-size=2
              - --enable-prefix-caching
              - --enable-chunked-prefill=false
              - --disable-log-requests
              - --kv-cache-dtype=fp8
              # Enforce eager wasn't supported with FLASHINFER.
              - --enforce-eager
            resourceProfile: nvidia-gpu-l4:8
            targetRequests: 500
          llama-3.1-8b-instruct-fp8-l4:
            enabled: true
          llama-4-maverick-430k-h100:
            enabled: false
          llama-4-maverick-384k-l40s:
            enabled: true
            features: [ TextGeneration ]
            url: hf://meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
            engine: VLLM
            env:
              VLLM_DISABLE_COMPILE_CACHE: "1"
            args:
              - --max-model-len=384000
              - --tensor-parallel-size=8
              - --enable-prefix-caching
              - --disable-log-requests
            resourceProfile: nvidia-gpu-l40s:8
          qwen3-235b-a22b-h100:
            enabled: false
            features: [ TextGeneration ]
            url: hf://Qwen/Qwen3-235B-A22B
            engine: VLLM
            env:
              VLLM_DISABLE_COMPILE_CACHE: "1"
            args:
              - --max-model-len=257000
              - --tensor-parallel-size=8
              - --enable-prefix-caching
              - --disable-log-requests
            resourceProfile: nvidia-gpu-h100:8
          qwen2.5-coder-1.5b-a10:
            enabled: false
            features: [ "TextGeneration" ]
            url: "hf://Qwen/Qwen2.5-Coder-1.5B-Instruct"
            engine: VLLM
            env:
              VLLM_ATTENTION_BACKEND: FLASHINFER
            args:
              - --max-model-len=2048
              - --max-num-seqs=16
              - --quantization=fp8
              - --kv-cache-dtype=fp8
            resourceProfile: nvidia-gpu-a10-24gb:1
            minReplicas: 0
          llama-3.1-405b-instruct-fp8-h100:
            enabled: false
            features: [ TextGeneration ]
            url: hf://neuralmagic/Meta-Llama-3.1-405B-Instruct-FP8
            engine: VLLM
            args:
              - --max-model-len=65536
              - --max-num-batched-token=65536
              - --gpu-memory-utilization=0.9
              - --tensor-parallel-size=8
              - --enable-prefix-caching
              - --disable-log-requests
              - --max-num-seqs=1024
              - --kv-cache-dtype=fp8
            # You can also use nvidia-gpu-a100-80gb:8
            resourceProfile: nvidia-gpu-h100:8
            targetRequests: 500
  
  destination:
    server: https://kubernetes.default.svc
    namespace: kubeai
  
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
  
  revisionHistoryLimit: 3