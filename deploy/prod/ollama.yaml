---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: gemma3-ollama
  namespace: argocd
spec:
  project: application
  source:
    repoURL: https://otwld.github.io/ollama-helm
    chart: ollama
    targetRevision: 1.14.0
    helm:
      releaseName: gemma3-ollama
      valuesObject:
        podLabels:
          app: gemma3-ollama
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              - labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - gemma3-ollama
                topologyKey: "kubernetes.io/hostname"
        persistentVolume:
          enabled: true
          existingClaim: "ollama-model"
        tolerations:
          - key: "ollama-node"
            operator: "Equal"
            value: "true"
            effect: "NoSchedule"
        autoscaling:
          enabled: true
          minReplicas: 1
          maxReplicas: 2
          targetCPUUtilizationPercentage: 80
        ollama:
          models:
            pull:
              - gemma3:12b-it-qat # 8.1GB
            run:
              - gemma3:12b-it-qat
          gpu:
            enabled: true
  
  destination:
    server: https://kubernetes.default.svc
    namespace: kivoyo
  
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
  
  revisionHistoryLimit: 3
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: qwen2-5-coder-instruct-ollama
  namespace: argocd
spec:
  project: application
  source:
    repoURL: https://otwld.github.io/ollama-helm
    chart: ollama
    targetRevision: 1.14.0
    helm:
      releaseName: qwen2-5-coder-instruct-ollama
      valuesObject:
        podLabels:
          app: qwen2-5-coder-instruct-ollama
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              - labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - qwen2-5-coder-instruct-ollama
                topologyKey: "kubernetes.io/hostname"
        persistentVolume:
          enabled: true
          existingClaim: "ollama-model"
        tolerations:
          - key: "ollama-node"
            operator: "Equal"
            value: "true"
            effect: "NoSchedule"
        autoscaling:
          enabled: true
          minReplicas: 1
          maxReplicas: 2
          targetCPUUtilizationPercentage: 80
        ollama:
          models:
            pull:
              - qwen2.5-coder:14b-instruct-q4_K_M # 9.0GB
            run:
              - qwen2.5-coder:14b-instruct-q4_K_M
          gpu:
            enabled: true
  
  destination:
    server: https://kubernetes.default.svc
    namespace: kivoyo
  
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
  
  revisionHistoryLimit: 3
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: llama3-1-ollama
  namespace: argocd
spec:
  project: application
  source:
    repoURL: https://otwld.github.io/ollama-helm
    chart: ollama
    targetRevision: 1.14.0
    helm:
      releaseName: llama3-1-ollama
      valuesObject:
        podLabels:
          app: llama3-1-ollama
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              - labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - llama3-1-ollama
                topologyKey: "kubernetes.io/hostname"
        persistentVolume:
          enabled: true
          existingClaim: "ollama-model"
        tolerations:
          - key: "ollama-node"
            operator: "Equal"
            value: "true"
            effect: "NoSchedule"
        autoscaling:
          enabled: true
          minReplicas: 1
          maxReplicas: 2
          targetCPUUtilizationPercentage: 80
        ollama:
          models:
            pull:
              - llama3.1:8b # 4.9GB
            run:
              - llama3.1:8b
          gpu:
            enabled: true
  
  destination:
    server: https://kubernetes.default.svc
    namespace: kivoyo
  
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
  
  revisionHistoryLimit: 3
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: llama3-1-instruct-ollama
  namespace: argocd
spec:
  project: application
  source:
    repoURL: https://otwld.github.io/ollama-helm
    chart: ollama
    targetRevision: 1.14.0
    helm:
      releaseName: llama3-1-instruct-ollama
      valuesObject:
        podLabels:
          app: llama3-1-instruct-ollama
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              - labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - llama3-1-instruct-ollama
                topologyKey: "kubernetes.io/hostname"
        persistentVolume:
          enabled: true
          existingClaim: "ollama-model"
        tolerations:
          - key: "ollama-node"
            operator: "Equal"
            value: "true"
            effect: "NoSchedule"
        autoscaling:
          enabled: true
          minReplicas: 1
          maxReplicas: 2
          targetCPUUtilizationPercentage: 80
        ollama:
          models:
            pull:
              - llama3.1:8b-instruct-q4_K_M # 4.9GB
            run:
              - llama3.1:8b-instruct-q4_K_M
          gpu:
            enabled: true
  
  destination:
    server: https://kubernetes.default.svc
    namespace: kivoyo
  
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
  
  revisionHistoryLimit: 3